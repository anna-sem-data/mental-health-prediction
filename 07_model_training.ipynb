{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80d4b408-5220-40e3-af89-6778761dfc6f",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43815296-1338-49c7-a203-3b8bcb309100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "19f14697-e50c-4cd7-9f80-6363e02f3029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in the dataset:\n",
      "Index(['statement', 'status', 'sentiment_score', 'sentiment_category',\n",
      "       'text_length', 'word_count', 'avg_word_length', 'stopword_count',\n",
      "       'first_person_pronoun_count', 'keywords_found', 'keyword_count',\n",
      "       'predicted_emotion'],\n",
      "      dtype='object')\n",
      "\n",
      "First few rows:\n",
      "                                           statement   status  \\\n",
      "0                                         oh my gosh  Anxiety   \n",
      "1  trouble sleeping confused mind restless heart ...  Anxiety   \n",
      "2  all wrong back off dear forward doubt stay in ...  Anxiety   \n",
      "3  ive shifted my focus to something else but im ...  Anxiety   \n",
      "4  im restless and restless its been a month now ...  Anxiety   \n",
      "\n",
      "   sentiment_score sentiment_category  text_length  word_count  \\\n",
      "0           0.0000            neutral           10           3   \n",
      "1          -0.7269           negative           61          10   \n",
      "2          -0.7351           negative           75          14   \n",
      "3          -0.4215           negative           59          11   \n",
      "4          -0.4939           negative           66          14   \n",
      "\n",
      "   avg_word_length  stopword_count  first_person_pronoun_count keywords_found  \\\n",
      "0                3               1                           1            NaN   \n",
      "1                6               3                           0            NaN   \n",
      "2                5               5                           0            NaN   \n",
      "3                5               3                           1            NaN   \n",
      "4                4               8                           0            NaN   \n",
      "\n",
      "   keyword_count predicted_emotion  \n",
      "0              0          surprise  \n",
      "1              0              fear  \n",
      "2              0              fear  \n",
      "3              0         Uncertain  \n",
      "4              0              fear  \n"
     ]
    }
   ],
   "source": [
    "# Load the CSV dataset\n",
    "data = pd.read_csv('emotion_data.csv')\n",
    "\n",
    "# Print the column names and first few rows to verify the structure\n",
    "print(\"Columns in the dataset:\")\n",
    "print(data.columns)\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f384de-5a03-40e1-bb6b-8ff944de7ccd",
   "metadata": {},
   "source": [
    "### Training the Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9241ef3-efd6-4e1b-affb-99d270948f9d",
   "metadata": {},
   "source": [
    "In this setup, our target variable is the predicted_emotion column. Essentially, I am building models to predict the emotion label assigned to each record in the dataset. By leveraging various features, from numeric scores like sentiment_score and text_length to categorical details like sentiment_category and text features like the statement itsel, the models learn to classify the underlying emotion of each text entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f8ec11bf-d8a4-41e1-b2d9-f8840201209f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features:\n",
      "   sentiment_score  text_length  word_count  avg_word_length  stopword_count  \\\n",
      "0           0.0000           10           3                3               1   \n",
      "1          -0.7269           61          10                6               3   \n",
      "2          -0.7351           75          14                5               5   \n",
      "3          -0.4215           59          11                5               3   \n",
      "4          -0.4939           66          14                4               8   \n",
      "\n",
      "   first_person_pronoun_count  keyword_count  \n",
      "0                           1              0  \n",
      "1                           0              0  \n",
      "2                           0              0  \n",
      "3                           1              0  \n",
      "4                           0              0  \n",
      "\n",
      "Target Variable:\n",
      "0     surprise\n",
      "1         fear\n",
      "2         fear\n",
      "3    Uncertain\n",
      "4         fear\n",
      "Name: predicted_emotion, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Define the target variable\n",
    "target_col = 'predicted_emotion'\n",
    "\n",
    "# Select numeric features (update the list if you want to include/exclude other columns)\n",
    "feature_cols = [\n",
    "    'sentiment_score', 'text_length', 'word_count', \n",
    "    'avg_word_length', 'stopword_count', \n",
    "    'first_person_pronoun_count', 'keyword_count'\n",
    "]\n",
    "\n",
    "X = data[feature_cols]\n",
    "y = data[target_col]\n",
    "\n",
    "print(\"Selected Features:\")\n",
    "print(X.head())\n",
    "print(\"\\nTarget Variable:\")\n",
    "print(y.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a3817a-9d8d-4073-a993-7caf3f3272dd",
   "metadata": {},
   "source": [
    "I split the dataset into 80% training and 20% testing because I wanted to have a robust set of data to train my model while still keeping a separate portion to evaluate its performance on unseen examples. This 80/20 split is a common approach that gives me enough data to learn patterns effectively, while ensuring that the test set is large enough to provide a realistic assessment of how well my model generalizes to new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ec2ecc7-d1da-4686-aefa-33ab1a97fa3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47c5ea2-0442-48ee-937f-ae8c406806c3",
   "metadata": {},
   "source": [
    "In my initial approach, I focused on leveraging key numeric features, such as sentiment score, text length, word count, average word length, stopword count, first-person pronoun count, and keyword countâ€”to capture important signals within the text that might indicate different emotions. I split the dataset into training and testing sets and applied feature scaling to ensure that all features contributed equally during model training. Then, I built five baseline models (Logistic Regression, Decision Tree, Random Forest, SVM, and K-Nearest Neighbors) to evaluate various algorithmic approaches and establish a performance benchmark. This process provided valuable insights into the predictive power of my engineered features and set the stage for further refinements and the exploration of more complex models, like neural networks, later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "40460112-b00c-436d-9c24-ccd4450af542",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\annas\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\annas\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\annas\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.7007\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Uncertain       0.71      0.97      0.82      6553\n",
      "       anger       0.00      0.00      0.00       167\n",
      "     disgust       0.00      0.00      0.00         9\n",
      "        fear       0.41      0.02      0.04       668\n",
      "         joy       0.00      0.00      0.00       232\n",
      "     neutral       0.00      0.00      0.00        12\n",
      "     sadness       0.31      0.06      0.10      1520\n",
      "    surprise       0.00      0.00      0.00        69\n",
      "\n",
      "    accuracy                           0.70      9230\n",
      "   macro avg       0.18      0.13      0.12      9230\n",
      "weighted avg       0.59      0.70      0.60      9230\n",
      "\n",
      "-------------------------------------\n",
      "Decision Tree Accuracy: 0.5804\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Uncertain       0.75      0.73      0.74      6553\n",
      "       anger       0.02      0.02      0.02       167\n",
      "     disgust       0.00      0.00      0.00         9\n",
      "        fear       0.13      0.14      0.13       668\n",
      "         joy       0.12      0.14      0.13       232\n",
      "     neutral       0.00      0.00      0.00        12\n",
      "     sadness       0.26      0.27      0.27      1520\n",
      "    surprise       0.02      0.01      0.02        69\n",
      "\n",
      "    accuracy                           0.58      9230\n",
      "   macro avg       0.16      0.17      0.16      9230\n",
      "weighted avg       0.59      0.58      0.58      9230\n",
      "\n",
      "-------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\annas\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\annas\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\annas\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.6928\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Uncertain       0.72      0.94      0.82      6553\n",
      "       anger       0.00      0.00      0.00       167\n",
      "     disgust       0.00      0.00      0.00         9\n",
      "        fear       0.29      0.04      0.07       668\n",
      "         joy       0.21      0.04      0.07       232\n",
      "     neutral       0.00      0.00      0.00        12\n",
      "     sadness       0.36      0.14      0.20      1520\n",
      "    surprise       0.00      0.00      0.00        69\n",
      "\n",
      "    accuracy                           0.69      9230\n",
      "   macro avg       0.20      0.15      0.15      9230\n",
      "weighted avg       0.60      0.69      0.62      9230\n",
      "\n",
      "-------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\annas\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\annas\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\annas\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vector Machine Accuracy: 0.7100\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Uncertain       0.71      1.00      0.83      6553\n",
      "       anger       0.00      0.00      0.00       167\n",
      "     disgust       0.00      0.00      0.00         9\n",
      "        fear       0.00      0.00      0.00       668\n",
      "         joy       0.00      0.00      0.00       232\n",
      "     neutral       0.00      0.00      0.00        12\n",
      "     sadness       0.00      0.00      0.00      1520\n",
      "    surprise       0.00      0.00      0.00        69\n",
      "\n",
      "    accuracy                           0.71      9230\n",
      "   macro avg       0.09      0.12      0.10      9230\n",
      "weighted avg       0.50      0.71      0.59      9230\n",
      "\n",
      "-------------------------------------\n",
      "K-Nearest Neighbors Accuracy: 0.6817\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Uncertain       0.73      0.92      0.81      6553\n",
      "       anger       0.00      0.00      0.00       167\n",
      "     disgust       0.00      0.00      0.00         9\n",
      "        fear       0.19      0.05      0.08       668\n",
      "         joy       0.33      0.06      0.10       232\n",
      "     neutral       0.00      0.00      0.00        12\n",
      "     sadness       0.32      0.15      0.20      1520\n",
      "    surprise       0.00      0.00      0.00        69\n",
      "\n",
      "    accuracy                           0.68      9230\n",
      "   macro avg       0.19      0.15      0.15      9230\n",
      "weighted avg       0.59      0.68      0.62      9230\n",
      "\n",
      "-------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\annas\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\annas\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\annas\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Define the models with a fixed random_state for reproducibility\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42),\n",
    "    'Support Vector Machine': SVC(random_state=42),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier()\n",
    "}\n",
    "\n",
    "# Dictionary to store model results\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Train the model using the scaled training data\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    # Evaluate model performance\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    \n",
    "    # Store the results\n",
    "    results[name] = {'accuracy': accuracy, 'report': report}\n",
    "    \n",
    "    # Print performance metrics\n",
    "    print(f'{name} Accuracy: {accuracy:.4f}')\n",
    "    print(report)\n",
    "    print('-------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cb84f96e-338b-4d9c-a43e-482076710ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Tuning and evaluating Logistic Regression ---\n",
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\annas\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\annas\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\annas\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression - Best Parameters: {'classifier__C': 1}\n",
      "Logistic Regression - Test Accuracy: 0.7542\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Uncertain       0.78      0.93      0.85      6553\n",
      "       anger       0.55      0.04      0.07       167\n",
      "     disgust       0.00      0.00      0.00         9\n",
      "        fear       0.63      0.34      0.44       668\n",
      "         joy       0.53      0.07      0.13       232\n",
      "     neutral       0.00      0.00      0.00        12\n",
      "     sadness       0.64      0.40      0.49      1520\n",
      "    surprise       0.00      0.00      0.00        69\n",
      "\n",
      "    accuracy                           0.75      9230\n",
      "   macro avg       0.39      0.22      0.25      9230\n",
      "weighted avg       0.72      0.75      0.72      9230\n",
      "\n",
      "-------------------------------------\n",
      "--- Tuning and evaluating Decision Tree ---\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\annas\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\annas\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\annas\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree - Best Parameters: {'classifier__max_depth': 5, 'classifier__min_samples_split': 2}\n",
      "Decision Tree - Test Accuracy: 0.7249\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Uncertain       0.74      0.96      0.83      6553\n",
      "       anger       0.00      0.00      0.00       167\n",
      "     disgust       0.00      0.00      0.00         9\n",
      "        fear       0.59      0.17      0.27       668\n",
      "         joy       0.00      0.00      0.00       232\n",
      "     neutral       0.00      0.00      0.00        12\n",
      "     sadness       0.58      0.19      0.29      1520\n",
      "    surprise       0.00      0.00      0.00        69\n",
      "\n",
      "    accuracy                           0.72      9230\n",
      "   macro avg       0.24      0.17      0.17      9230\n",
      "weighted avg       0.66      0.72      0.66      9230\n",
      "\n",
      "-------------------------------------\n",
      "--- Tuning and evaluating Random Forest ---\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\annas\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\annas\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\annas\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest - Best Parameters: {'classifier__max_depth': None, 'classifier__n_estimators': 100}\n",
      "Random Forest - Test Accuracy: 0.7432\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Uncertain       0.75      0.97      0.84      6553\n",
      "       anger       0.00      0.00      0.00       167\n",
      "     disgust       0.00      0.00      0.00         9\n",
      "        fear       0.75      0.26      0.38       668\n",
      "         joy       0.75      0.01      0.03       232\n",
      "     neutral       0.00      0.00      0.00        12\n",
      "     sadness       0.67      0.22      0.33      1520\n",
      "    surprise       0.50      0.01      0.03        69\n",
      "\n",
      "    accuracy                           0.74      9230\n",
      "   macro avg       0.43      0.18      0.20      9230\n",
      "weighted avg       0.72      0.74      0.68      9230\n",
      "\n",
      "-------------------------------------\n",
      "--- Tuning and evaluating Support Vector Machine ---\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\annas\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\annas\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\annas\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vector Machine - Best Parameters: {'classifier__C': 1, 'classifier__kernel': 'linear'}\n",
      "Support Vector Machine - Test Accuracy: 0.7494\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Uncertain       0.76      0.95      0.85      6553\n",
      "       anger       0.00      0.00      0.00       167\n",
      "     disgust       0.00      0.00      0.00         9\n",
      "        fear       0.66      0.29      0.40       668\n",
      "         joy       0.00      0.00      0.00       232\n",
      "     neutral       0.00      0.00      0.00        12\n",
      "     sadness       0.66      0.32      0.43      1520\n",
      "    surprise       0.00      0.00      0.00        69\n",
      "\n",
      "    accuracy                           0.75      9230\n",
      "   macro avg       0.26      0.19      0.21      9230\n",
      "weighted avg       0.70      0.75      0.70      9230\n",
      "\n",
      "-------------------------------------\n",
      "--- Tuning and evaluating K-Nearest Neighbors ---\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\annas\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\annas\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\annas\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Nearest Neighbors - Best Parameters: {'classifier__n_neighbors': 5, 'classifier__weights': 'uniform'}\n",
      "K-Nearest Neighbors - Test Accuracy: 0.7109\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Uncertain       0.75      0.91      0.82      6553\n",
      "       anger       0.00      0.00      0.00       167\n",
      "     disgust       0.00      0.00      0.00         9\n",
      "        fear       0.54      0.31      0.39       668\n",
      "         joy       0.36      0.05      0.09       232\n",
      "     neutral       0.00      0.00      0.00        12\n",
      "     sadness       0.44      0.27      0.33      1520\n",
      "    surprise       0.00      0.00      0.00        69\n",
      "\n",
      "    accuracy                           0.71      9230\n",
      "   macro avg       0.26      0.19      0.21      9230\n",
      "weighted avg       0.66      0.71      0.67      9230\n",
      "\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Fine-tuning the parameters to increase accuracy\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Re-create the feature matrix X with all desired columns\n",
    "feature_cols = ['sentiment_score', 'text_length', 'word_count', \n",
    "                'avg_word_length', 'stopword_count', \n",
    "                'first_person_pronoun_count', 'keyword_count', \n",
    "                'sentiment_category', 'status', 'statement', 'keywords_found']\n",
    "X = data[feature_cols]\n",
    "y = data['predicted_emotion']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Work on copies and fill missing text values\n",
    "X_train = X_train.copy()\n",
    "X_test = X_test.copy()\n",
    "for col in ['statement', 'keywords_found']:\n",
    "    X_train[col] = X_train[col].fillna('')\n",
    "    X_test[col] = X_test[col].fillna('')\n",
    "\n",
    "# Define the preprocessor to handle each feature type\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), ['sentiment_score', 'text_length', 'word_count', \n",
    "                                     'avg_word_length', 'stopword_count', \n",
    "                                     'first_person_pronoun_count', 'keyword_count']),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), ['sentiment_category', 'status']),\n",
    "        ('txt_stmt', TfidfVectorizer(max_features=500), 'statement'),\n",
    "        ('txt_kw', TfidfVectorizer(max_features=100), 'keywords_found')\n",
    "    ])\n",
    "\n",
    "# Define models with a more concise parameter grid for faster tuning\n",
    "models = {\n",
    "    'Logistic Regression': (\n",
    "        LogisticRegression(max_iter=1000, random_state=42),\n",
    "        {'classifier__C': [0.1, 1]}\n",
    "    ),\n",
    "    'Decision Tree': (\n",
    "        DecisionTreeClassifier(random_state=42),\n",
    "        {'classifier__max_depth': [None, 5], \n",
    "         'classifier__min_samples_split': [2, 5]}\n",
    "    ),\n",
    "    'Random Forest': (\n",
    "        RandomForestClassifier(random_state=42),\n",
    "        {'classifier__n_estimators': [50, 100], \n",
    "         'classifier__max_depth': [None, 10]}\n",
    "    ),\n",
    "    'Support Vector Machine': (\n",
    "        SVC(random_state=42),\n",
    "        {'classifier__C': [0.1, 1], \n",
    "         'classifier__kernel': ['linear', 'rbf']}\n",
    "    ),\n",
    "    'K-Nearest Neighbors': (\n",
    "        KNeighborsClassifier(),\n",
    "        {'classifier__n_neighbors': [3, 5], \n",
    "         'classifier__weights': ['uniform', 'distance']}\n",
    "    )\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "# Loop through each model, performing a simplified grid search and evaluation\n",
    "for name, (model, param_grid) in models.items():\n",
    "    print(f\"--- Tuning and evaluating {name} ---\")\n",
    "    \n",
    "    # Build the pipeline: preprocessor then classifier\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', model)\n",
    "    ])\n",
    "    \n",
    "    # Use GridSearchCV with fewer parameter options and 3-fold CV for speed\n",
    "    grid_search = GridSearchCV(pipeline, param_grid, cv=3, scoring='accuracy', n_jobs=-1, verbose=1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    best_model = grid_search.best_estimator_\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    \n",
    "    results[name] = {'accuracy': acc, 'report': report, 'best_params': grid_search.best_params_}\n",
    "    \n",
    "    print(f\"{name} - Best Parameters: {grid_search.best_params_}\")\n",
    "    print(f\"{name} - Test Accuracy: {acc:.4f}\")\n",
    "    print(report)\n",
    "    print('-------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97c30c1-fcc4-4f6b-928d-6f68652dd00f",
   "metadata": {},
   "source": [
    "I decided to include XGBoost in my analysis because of its proven efficiency and effectiveness in handling complex datasets. Its ability to build ensembles of decision trees using gradient boosting allows for robust handling of non-linear relationships and interactions between features. Given the variety of models I was comparing, I wanted to see if XGBoost could provide a performance boost, especially in terms of accuracy and generalization, by leveraging its regularization techniques to reduce overfitting. This approach was particularly appealing as it helped me validate the strength of my engineered features and explore a more advanced method beyond the baseline algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6a8589b0-3a47-402f-98f0-c44a29264aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\users\\annas\\anaconda3\\lib\\site-packages (2.1.4)\n",
      "Requirement already satisfied: numpy in c:\\users\\annas\\anaconda3\\lib\\site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\annas\\anaconda3\\lib\\site-packages (from xgboost) (1.13.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\annas\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [07:20:13] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Accuracy: 0.7029\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Uncertain       0.72      0.97      0.83      6553\n",
      "       anger       0.00      0.00      0.00       167\n",
      "     disgust       0.00      0.00      0.00         9\n",
      "        fear       0.36      0.02      0.04       668\n",
      "         joy       0.24      0.03      0.05       232\n",
      "     neutral       0.00      0.00      0.00        12\n",
      "     sadness       0.37      0.08      0.13      1520\n",
      "    surprise       1.00      0.01      0.03        69\n",
      "\n",
      "    accuracy                           0.70      9230\n",
      "   macro avg       0.34      0.14      0.13      9230\n",
      "weighted avg       0.61      0.70      0.61      9230\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# XGBoost Model Training\n",
    "\n",
    "!pip install xgboost\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Encode target variable to numeric labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# Define the XGBoost classifier\n",
    "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)\n",
    "\n",
    "# Train the XGBoost model using the scaled training data\n",
    "xgb_model.fit(X_train_scaled, y_train_encoded)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_xgb = xgb_model.predict(X_test_scaled)\n",
    "\n",
    "# Decode predictions back to original labels if needed\n",
    "y_pred_labels = label_encoder.inverse_transform(y_pred_xgb)\n",
    "\n",
    "# Evaluate model performance\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "accuracy_xgb = accuracy_score(y_test_encoded, y_pred_xgb)\n",
    "report_xgb = classification_report(y_test_encoded, y_pred_xgb, target_names=label_encoder.classes_)\n",
    "\n",
    "print('XGBoost Accuracy: {:.4f}'.format(accuracy_xgb))\n",
    "print(report_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "33599c47-ef20-4d51-a0e7-57f2026f1a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 108 candidates, totalling 324 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\annas\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [10:41:13] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found: {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 300, 'subsample': 0.8}\n",
      "Best cross-validation accuracy: 0.7095105400573779\n",
      "Tuned XGBoost Test Accuracy: 0.7102\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Uncertain       0.71      1.00      0.83      6553\n",
      "       anger       0.00      0.00      0.00       167\n",
      "     disgust       0.00      0.00      0.00         9\n",
      "        fear       1.00      0.00      0.01       668\n",
      "         joy       1.00      0.00      0.01       232\n",
      "     neutral       0.00      0.00      0.00        12\n",
      "     sadness       0.44      0.01      0.02      1520\n",
      "    surprise       0.00      0.00      0.00        69\n",
      "\n",
      "    accuracy                           0.71      9230\n",
      "   macro avg       0.39      0.13      0.11      9230\n",
      "weighted avg       0.68      0.71      0.59      9230\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\annas\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\annas\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\annas\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Fine-tuning the XGBoost parameters to improve the model's accuracy\n",
    "\n",
    "# Encode target variable to numeric labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# Define the XGBoost classifier\n",
    "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)\n",
    "\n",
    "# Define a parameter grid for fine-tuning\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV to search for the best hyperparameters\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    param_grid=param_grid,\n",
    "    cv=3,  # You can adjust the number of folds\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Run grid search on the training data\n",
    "grid_search.fit(X_train_scaled, y_train_encoded)\n",
    "\n",
    "# Print the best parameters and best score from grid search\n",
    "print(\"Best parameters found:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation accuracy:\", grid_search.best_score_)\n",
    "\n",
    "# Evaluate the best estimator on the test set\n",
    "best_xgb = grid_search.best_estimator_\n",
    "y_pred_best = best_xgb.predict(X_test_scaled)\n",
    "\n",
    "accuracy_best = accuracy_score(y_test_encoded, y_pred_best)\n",
    "print(\"Tuned XGBoost Test Accuracy: {:.4f}\".format(accuracy_best))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_encoded, y_pred_best, target_names=label_encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037af1c3-4648-44df-aa19-bafc9364c9ef",
   "metadata": {},
   "source": [
    "### Comparison Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb5e6b6-e8da-42e4-81fc-24dda8720df5",
   "metadata": {},
   "source": [
    "In the first round, I trained five baseline models (Logistic Regression, Decision Tree, Random Forest, SVM, and K-Nearest Neighbors) using basic feature preprocessing. This initial evaluation provided a quick benchmark, though it highlighted issues such as overfitting in tree-based models and inconsistent performance across classifiers.\n",
    "\n",
    "In the next round, I expanded the feature set by incorporating numeric, categorical, and text-based data through a ColumnTransformer, and applied TFâ€‘IDF vectorization on text features. With a more refined preprocessing pipeline and a simplified grid search for hyperparameter tuning, all models showed modest accuracy improvements compared to their baseline versions.\n",
    "\n",
    "Comparing the two rounds of XGBoost, the initial run served as a baseline while the tuned versionâ€”despite a longer runtimeâ€”yielded a slight accuracy gain (around 1% increase). This suggests that while XGBoost can benefit from tuning, the improvements may be incremental without further feature engineering or addressing class imbalance.\n",
    "\n",
    "Overall, Random Forest and XGBoost emerged as top performers in terms of accuracy and handling complex feature interactions, whereas Logistic Regression and SVM remain attractive due to their lower computational cost. Each approach has its strengths and limitations, and further fine-tuning or ensemble methods may be required to fully optimize predictive performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
